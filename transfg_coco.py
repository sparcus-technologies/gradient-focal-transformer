# -*- coding: utf-8 -*-
"""TransFG_COCO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WgF4HTV31uTXMaOzm2R8ZZe7DX-8PfN5
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torchvision.transforms import InterpolationMode
import torchvision.datasets as datasets
import os
import numpy as np
from PIL import Image
from timm import create_model
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from tqdm import tqdm

class TransFGBlock(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class TransFG(nn.Module):
    def __init__(self, base_model='vit_base_patch16_224', num_classes=80, patch_size=16):
        super().__init__()
        self.base_model = create_model(base_model, pretrained=True)

        self.num_patches = (224 // patch_size) ** 2
        self.patch_size = patch_size
        self.num_classes = num_classes

        # Part selection module
        self.part_select = nn.Sequential(
            nn.Linear(768, 384),
            nn.ReLU(),
            nn.Linear(384, 1)
        )

        # Local-global transformer blocks
        self.local_blocks = nn.ModuleList([
            TransFGBlock(dim=768, num_heads=12)
            for _ in range(2)
        ])

        # Final classifier
        self.classifier = nn.Linear(768, num_classes)

    def forward(self, x):
        # Get patch embeddings from base ViT
        x = self.base_model.patch_embed(x)
        cls_token = self.base_model.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        x = self.base_model.pos_drop(x + self.base_model.pos_embed)

        # Apply base transformer blocks
        for blk in self.base_model.blocks[:8]:  # Use first 8 blocks
            x = blk(x)

        # Part selection
        patch_tokens = x[:, 1:]  # Remove CLS token
        part_scores = self.part_select(patch_tokens).squeeze(-1)
        part_attention = F.softmax(part_scores, dim=-1)

        # Select top-K patches (K=100)
        K = min(100, part_attention.size(1))  # Ensure K isn't larger than the number of patches
        _, top_k_idx = torch.topk(part_attention, K, dim=1)
        batch_idx = torch.arange(x.size(0)).unsqueeze(1).expand(-1, K).to(x.device)
        selected_patches = patch_tokens[batch_idx, top_k_idx]

        # Add CLS token back
        x = torch.cat((x[:, :1], selected_patches), dim=1)

        # Apply local-global transformer blocks
        for blk in self.local_blocks:
            x = blk(x)

        # Classification
        x = x[:, 0]  # Use CLS token
        x = self.classifier(x)
        return x

# Custom COCO Classification Dataset (wraps CocoDetection)
class CocoClassification(Dataset):
    def __init__(self, root, annFile, transform=None):
        self.dataset = datasets.CocoDetection(root=root, annFile=annFile)
        self.transform = transform

        # Get categories from COCO dataset
        self.categories = self.dataset.coco.loadCats(self.dataset.coco.getCatIds())
        self.cat_ids = [cat['id'] for cat in self.categories]
        self.cat_id_to_label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}
        self.classes = [cat['name'] for cat in self.categories]

        # Filter images to only include those with annotations
        self.valid_ids = []
        for idx in range(len(self.dataset)):
            img, anns = self.dataset[idx]
            if len(anns) > 0:
                self.valid_ids.append(idx)

        print(f"Found {len(self.valid_ids)} images with annotations out of {len(self.dataset)} total images")

    def __getitem__(self, idx):
        # Get the image and annotations from the filtered indices
        real_idx = self.valid_ids[idx]
        img, anns = self.dataset[real_idx]

        # Get the primary category (use the first annotation's category)
        cat_id = anns[0]['category_id']
        label = self.cat_id_to_label[cat_id]

        # Apply transform if specified
        if self.transform is not None:
            img = self.transform(img)

        return img, label

    def __len__(self):
        return len(self.valid_ids)

# Training setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Data transforms
train_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),
    transforms.RandomHorizontalFlip(),
    transforms.RandomAutocontrast(),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Download COCO dataset
print("Setting up COCO dataset...")
os.makedirs('./data/coco', exist_ok=True)

# Automatically download COCO dataset if not present
if not os.path.exists('./data/coco/annotations/instances_train2017.json'):
    print("Downloading COCO annotations...")
    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P ./data/coco/
    !unzip -q ./data/coco/annotations_trainval2017.zip -d ./data/coco/

if not os.path.exists('./data/coco/train2017'):
    print("Downloading COCO train2017 images...")
    !wget http://images.cocodataset.org/zips/train2017.zip -P ./data/coco/
    !unzip -q ./data/coco/train2017.zip -d ./data/coco/

if not os.path.exists('./data/coco/val2017'):
    print("Downloading COCO val2017 images...")
    !wget http://images.cocodataset.org/zips/val2017.zip -P ./data/coco/
    !unzip -q ./data/coco/val2017.zip -d ./data/coco/

# COCO dataset paths
train_img_dir = './data/coco/train2017'
val_img_dir = './data/coco/val2017'
train_ann_file = './data/coco/annotations/instances_train2017.json'
val_ann_file = './data/coco/annotations/instances_val2017.json'

# Load COCO datasets
print("Loading COCO datasets...")
train_dataset = CocoClassification(root=train_img_dir, annFile=train_ann_file, transform=train_transform)
val_dataset = CocoClassification(root=val_img_dir, annFile=val_ann_file, transform=test_transform)

num_classes = len(train_dataset.classes)
print(f"Number of COCO classes: {num_classes}")

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

# Model initialization
model = TransFG(num_classes=num_classes)
model = model.to(device)

# Training parameters
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.05)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

def train_epoch(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for images, targets in tqdm(loader, desc="Training"):
        images, targets = images.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, targets)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    return total_loss / len(loader), 100. * correct / total

def evaluate(model, loader):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for images, targets in tqdm(loader, desc="Evaluating"):
            images = images.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)

    accuracy = accuracy_score(all_targets, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='weighted')

    return accuracy, precision, recall, f1

# Training loop
num_epochs = 20
best_val_acc = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)
    val_acc, val_precision, val_recall, val_f1 = evaluate(model, val_loader)

    scheduler.step()

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"Val Acc: {val_acc*100:.2f}%, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_transfg_coco.pth')

# Final evaluation
print("Loading best model for final evaluation...")
model.load_state_dict(torch.load('best_transfg_coco.pth'))
test_acc, test_precision, test_recall, test_f1 = evaluate(model, val_loader)

print("\nTest Set Results:")
print(f"Accuracy: {test_acc*100:.2f}%")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1-score: {test_f1:.4f}")